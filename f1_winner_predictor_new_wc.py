# -*- coding: utf-8 -*-
"""f1-winner-predictor-new-wc

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GxdrdGK67goJieWL9c09vPMiNhmGjdyf

# Formula 1 Winner Predictor - Data Preprocessing and Model Building

# **Step 1: Setup and Dependencies**
# Import required libraries and mount Google Drive for dataset access.
# Configure Kaggle API for dataset download.
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

from google.colab import drive
drive.mount('/content/drive')

!kaggle datasets download -d rohanrao/formula-1-world-championship-1950-2020

from zipfile import ZipFile
file_name = '/content/formula-1-world-championship-1950-2020.zip'
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('file extracted')

!ls

"""# **Step 2: Import Libraries**
# Load essential libraries for data manipulation, visualization, and machine learning.

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from imblearn.over_sampling import SMOTE

"""# **Step 3: Load and Explore Datasets**
# Load the Formula 1 datasets and merge them for preprocessing.

"""

extracted_folder = '/content/'
results_df = pd.read_csv(os.path.join(extracted_folder, 'results.csv'))
races_df = pd.read_csv(os.path.join(extracted_folder, 'races.csv'))
drivers_df = pd.read_csv(os.path.join(extracted_folder, 'drivers.csv'))
constructors_df = pd.read_csv(os.path.join(extracted_folder, 'constructors.csv'))

# Merge results with races for context
merged_df = pd.merge(results_df, races_df, on='raceId', how='left')

"""# **Step 4: Preprocess Data**
# Encode categorical features and engineer relevant features for the model.

"""

# Encode categorical features
driver_encoder = LabelEncoder()
constructor_encoder = LabelEncoder()
merged_df['driver_encoded'] = driver_encoder.fit_transform(merged_df['driverId'])
merged_df['constructor_encoded'] = constructor_encoder.fit_transform(merged_df['constructorId'])

"""# Handle rare driver occurrences"""

# Rare drivers: Assign special value
rare_driver_threshold = 10
rare_drivers = merged_df['driver_encoded'].value_counts()[merged_df['driver_encoded'].value_counts() < rare_driver_threshold].index
merged_df.loc[merged_df['driver_encoded'].isin(rare_drivers), 'driver_encoded'] = -1

"""# Feature engineering: Driver-constructor points and rolling average of points

"""

# Feature engineering
merged_df['driver_constructor_points'] = merged_df.groupby(['driver_encoded', 'constructor_encoded'])['points'].transform('mean')
merged_df['rolling_avg_points'] = merged_df.groupby('driver_encoded')['points'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())

"""# Define features and target for the model"""

# Select features and target
features = [
    'grid', 'laps', 'driver_constructor_points', 'rolling_avg_points',
    'constructor_encoded', 'year', 'round','fastestLapTime'
]

"""# **Helper Function:** Convert time strings to seconds"""

def time_to_seconds(time_str):
    if pd.isnull(time_str):
        return np.nan  # Handle NaN values by returning NaN
    if isinstance(time_str, (int, float)):
        return time_str  # Return as is if it is already a number
    if isinstance(time_str, str):
        parts = time_str.split(':')
        if len(parts) == 2:  # Check if the split results in two parts
            minutes, seconds_milliseconds = parts
            seconds, milliseconds = seconds_milliseconds.split('.')
            total_seconds = int(minutes) * 60 + int(seconds) + int(milliseconds) / 1000
            return total_seconds
        else:
            return np.nan # Handle anything that does not split into 2 parts as NaN
    else:
        return np.nan #Handle anything else as NaN

"""# Apply helper function to 'fastestLapTime' and handle missing values"""

X = merged_df[features]
y = merged_df['driver_encoded']

X['fastestLapTime'] = X['fastestLapTime'].apply(time_to_seconds)

# Handle missing values
X.fillna(0, inplace=True)

"""# **Step 5: Data Scaling and Oversampling**
# Scale features and balance the dataset using SMOTE.

"""

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encode target variable as categorical
y_encoded = to_categorical(y)

# Oversample using SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X_scaled, np.argmax(y_encoded, axis=1))

# Convert target back to categorical after resampling
y_resampled_categorical = to_categorical(y_resampled)

"""# **Step 6: Train-Test Split**
# Split the resampled data into training and testing sets.

"""

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled_categorical, test_size=0.2, random_state=42)

"""# **Step 7: Model Definition**
# Define and compile a neural network for winner prediction.

"""

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_resampled), y=y_resampled)
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

# Define model
model = Sequential([
    Dense(256, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(y_resampled_categorical.shape[1], activation='softmax')
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""# **Step 8: Training with Callbacks**
# Use EarlyStopping and ReduceLROnPlateau to enhance training efficiency.

"""

# Callbacks for early stopping and learning rate adjustment
early_stopping = EarlyStopping(monitor='val_loss', patience=10)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=35,
    batch_size=64,
    validation_split=0.2,
    class_weight=class_weights_dict,
    callbacks=[early_stopping, reduce_lr]
)

"""# **Step 9: Model Evaluation**
# Evaluate the model's performance on the test set and display predictions.

"""

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Predict on test data
predictions = np.argmax(model.predict(X_test), axis=1)
predicted_drivers = driver_encoder.inverse_transform(predictions)

"""# Display sample predictions"""

# Display sample predictions
sample_predictions = pd.DataFrame({
    'Actual Driver': driver_encoder.inverse_transform(np.argmax(y_test, axis=1)),
    'Predicted Driver': predicted_drivers
})

print(sample_predictions.head())

"""# **Step 10: Save Model**
# Save the trained model for future use.
"""

# Save the trained model
model.save('models/f1_driver_predictor.h5', save_format='keras_v3')
